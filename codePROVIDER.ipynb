{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkWP/UzjGeRDdB77u/iNHh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation de Apache Spark\n",
        "\n"
      ],
      "metadata": {
        "id": "nMy2A9qgE-Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installartion et configuration PySpark\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when, sha2, concat_ws\n",
        "\n",
        "# Initialisation de la session Spark\n",
        "spark = SparkSession.builder.appName(\"OMOP_Provider\").getOrCreate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eryVEUP_dPt",
        "outputId": "aa46be05-fe78-4cb2-b319-66b5f304ca87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Chargement des données"
      ],
      "metadata": {
        "id": "72Gl4x_5FZkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des tables IR_ACT_V et IR_SPE_V dans des dataframes spark\n",
        "ir_act_v = spark.read.csv(\"./ir_act_v (1).csv\", header=True, inferSchema=True)\n",
        "ir_spe_v = spark.read.csv(\"./ir_spe_v (1).csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Sélection des colonnes nécessaires et renom\n",
        "ir_act_v = ir_act_v.select(col(\"pfs_act_nat\").alias(\"specialty_source_value\"))\n",
        "ir_spe_v = ir_spe_v.select(col(\"pfs_spe_cod\").alias(\"specialty_source_value\"))\n",
        "\n",
        "# Union des deux sources\n",
        "specialties = ir_act_v.union(ir_spe_v).distinct()"
      ],
      "metadata": {
        "id": "SgE-f0cw_j70"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Transformation des données :"
      ],
      "metadata": {
        "id": "upqIAHDVFoDc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElAgJHwY_VbT",
        "outputId": "08513406-9f8b-42b7-942e-4a6136228f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+---------------------+--------------------+--------------------+\n",
            "|specialty_source_value|provider_source_value|         provider_id|specialty_concept_id|\n",
            "+----------------------+---------------------+--------------------+--------------------+\n",
            "|                    26|                   26|5f9c4ab08cac7457e...|            36682004|\n",
            "|                    50|                   50|1a6562590ef19d104...|            38003810|\n",
            "|                     6|                    6|e7f6c011776e8db7c...|            66862007|\n",
            "|                     1|                    1|6b86b273ff34fce19...|           112247003|\n",
            "+----------------------+---------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Génération de provider_source_value (identique à specialty_source_value)\n",
        "specialties = specialties.withColumn(\"provider_source_value\", col(\"specialty_source_value\"))\n",
        "\n",
        "# Génération de provider_id (hachage SHA2 de provider_source_value)\n",
        "specialties = specialties.withColumn(\"provider_id\", sha2(col(\"provider_source_value\").cast(\"STRING\"), 256))\n",
        "\n",
        "# Mapping des spécialités aux specialty_concept_id\n",
        "specialty_mapping_data = [\n",
        "    (50, 38003810),# pharmacien\n",
        "    (26, 36682004), #kiniséthérapeute\n",
        "    (6, 66862007), # médecin generaliste\n",
        "    (1, 112247003) # médecin généraliste\n",
        "]\n",
        "\n",
        "specialty_mapping_df = spark.createDataFrame(specialty_mapping_data, [\"specialty_source_value\", \"specialty_concept_id\"])\n",
        "\n",
        "# Joindre specialties avec specialty_mapping_df pour ajouter specialty_concept_id\n",
        "specialties = specialties.join(specialty_mapping_df, on=\"specialty_source_value\", how=\"left\")\n",
        "\n",
        "# Enregistrer le résultat en Parquet\n",
        "specialties.write.parquet(\"/path/to/output/provider.parquet\", mode=\"overwrite\")\n",
        "\n",
        "# Afficher le résultat final\n",
        "specialties.show()"
      ]
    }
  ]
}